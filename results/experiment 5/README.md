## Эксперимент №5: проверка последствий обрезания инпута до n примеров.

### Проблема

Пусть нам на вход приходит $N$ примеров и $1$ запрос. Тогда в обычной реализации на вход модели будет подано $2N + 1$ токен. Что будет, если мы обрежем $N$ до некоторого меньшего $n$? Это можно сделать множеством разных способов, и в данном эксперименте мы рассмотрим два:

1. Мы берём первые $n$ примеров из инпута.
2. Мы берём случайные $n$ примеров из инпута.

Можно было бы использовать дропаут после каждого Input Injection, но я решил делать обрезку тензора вручную.

Поскольку первый вариант идентичен простой обрезке контекста и его не имеет смысла тестировать, мы обрежем инпут после того, как пройдёт одна итерация с полным контекстом. Второй вариант мы реализуем таким же образом. 

Можно было бы рассмотреть ещё такие варианты:

- Брать не первые $n$ примеров, а последние $n$. Так как мы обрезаем инпут, который содержит информацию о позиционном кодировании, и выход итерации, который содержит также информацию о каузальных связях через механим внимания, это может изменить результаты. С другой стороны, примеры в инпуте расположены случайно и не имеют никакой информации в собственном порядке (они не отранжированы).
- Каждую новую итерацию подаётся всё меньше и меньше примеров.
- Обрезка происходит после того, как сформировался эмбеддинг инпута, а не после первой итерации.
- Обучать модель делать обрезку со случайным $n$.

### Ожидаемый результат

Поскольку мы тренируем модель на задачу Next Token Prediction и соответственно ожидаем определённую структуру предсказаний модели, первый вариант окажется эффективнее. Также он может оказаться эффективнее ещё и потому, что элемент случайности значительно усложнит тренировку модели.

### Дизайн эксперимента

Мы используем ту же модель, что и в предыдущем эксперименте. В данном случае в качестве Input Injection выбрано простое сложение, поскольку оно меньше всего зависит от случайности. Мы используем три зерна генерации: $42$, $451$ и $1984$. На этот раз мы ограничиваем контекст модели длиной $129$, т.е. в него влезет $64$ примера. Будем обучать модели с $b = 10$ при $n = 64$, $48$, $32$ и $24$. Размер батча для обоих моделей равен $64$, а $\text{lr}$ равен $5e-4$.

### Результаты

Результаты валидации представлены на рисунке. Отметим, что `Default` и `Random` при $n = 64$ абсолютно идентично вычисляются (т.к. на вход подаётся всего 64 примера, нам нет необходимости каким-либо образом изменять тензоры).

![](./performance%201.svg)

Нетрудно заметить, что случайный выбор значительно ухудшает качество. Скорее всего это связано с тем, что в модели значительный вклад вносит каузальная маска, а после того, как мы случайным образом выбрали элементы из выхода трансформера в котором закодирована некоторая позиционная информация, она соответственно теряется или становится гораздо более нечёткой для модели. Несмотря на то, что модели со случайным выбором действительно обучаются, их рассматривать, вероятно, не стоит.

Небольшое уменьшение числа токенов ожидаемо слабо повлияло на результативность модели. Возможно, что curriculum learning для $n$ или выбор случайного $n$ каждую итерацию сыграют в качестве регуляризации и смогут сделать модель гораздо более устойчивой к изменению числа подаваемых на вход токенов.

Сравним способность к экстраполяции у моделей:

![](./loss%20comparison.svg)

Интересным образом, обучение моделей с обрезкой контекста приводит к улучшению стабильности работы модели (это заметно по отклонению от среднего). Это может быть также связано и с неудачным зерном генерации, поэтому вот сравнение разных зёрен:

![](./extrap%20comparison.svg)

Как мы видим, модель с $n = 32$ при $b = 50$ показывает лучший результат, чем $n = 48$ и $n = 24$, и чуть хуже, чем $n = 64$.

Мы видим также, что в среднем модели с обрезкой $n$ (то есть модели, которые обучаются на меньшем числе примеров) лучше экстраполируются, а также то, что они хуже обучились под $b_{\text{train}}$. Соответственно, мы можем сказать, что это является некоторым свидетельством в сторону проверки гипотез 1 и 3 из эксперимента 1 - модели, судя по всему, действительно могут переобучиться под $b_{\text{train}}$, однако их способность к экстраполяции будто бы слабо зависит от числа тренировочных примеров.