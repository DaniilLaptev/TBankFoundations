## Эксперимент №2: изменение глубины vs изменение итераций.

### Проблема

Нам хотелось бы узнать, что будет, если зацикливать несколько слоёв. Авторы статьи получили такой результат, что увеличение $L$ ведёт к улучшению качества модели, что, в принципе, ожидаемо. Однако нам интересно следующее: как будут соотноситься модели, например, $(L=1, b=4)$, $(L=2, b=2)$ и $(L=4, b=1)$.

### Ожидаемый результат

Модели меньшего размера в среднем будут показывать себя хуже, хоть и смогут достичь уровня моделей большего размера.

### Дизайн эксперимента

Каждый слой в наших моделях устроен следующим образом:

| Параметр | Головы внимания | Скрытая размерность | Размерность MLP слоя | Функция активации |
|---|:---:|:---:|:---:|:---:|
| Значение | 4 | 64 | 256 | GELU |

Затем мы брали различные комбинации моделей $L$ и $b$ так, чтобы $L \times b \leq 8$. Все модели обучались одинаковое число итераций $(10000)$ с одинаковым размером батча $(64)$ и одинаковым числом in-context примеров $(N = 63)$. Модель с $L = 1$ мы обучаем с $\text{lr} = 1e-3$, а остальные с $\text{lr} = 5e-4$.

### Результаты

Сравнение результатов работы моделей представлено на графике. Способ получения данных описан в README репозитория.

![Результат](./performance%201.svg)

Нетрудно увидеть, что увеличение натуральной экспрессивности модели даёт гораздо больший прирост, нежели увеличение $b$. Возможно, что увеличив число итераций для $L = 2, 4$, мы бы могли достичь уровня модели с восемью слоями. В принципе, это также ожидаемый результат, поскольку увеличение $L$ приводит к кратному увеличению числа параметров и соответственно экспрессивности модели. 

Увеличение числа итераций приводит к кратному увеличению вычислительных ресурсов во время обучения. С другой стороны, во время инференса мы можем динамически подстраивать необходимое число итераций, особенно если наша модель хорошо выучила искомый алгоритм и хорошо экстраполируется. Если мы хотим больше итераций во время инференса, нам необходимо тренировать модель при большом $b$.

В то же время увеличение размера модели даёт лучший результат при меньшем числе итераций. Если взять две модели, А с большим числом параметров и Б с большим числом итераций, которые обучать одинаково дорого, и при этом А работает эффективнее чем Б, какую модель мы будем использовать на практике? Скорее всего А. С другой стороны, мы хотим сделать инференс дешевле, и поэтому нам гораздо легче будет запускать меньшую модель на каком-то динамически изменяемом числе итераций (например как в случае диффузионных моделей мы можем подстраивать число шагов денойзинга, что очень напоминает looped подход).

Не до конца ясно, в чём конкретно может быть выгода Looped Transformer. Возможно, есть ситуации, в которых будет более разумно использовать именно looped версию модели, нежели увеличивать число параметров (например в случае недостаточно большого количества данных, как мы выяснили из эксперимента 1 - большая модель хуже обучится, чем маленькая, и хуже экстраполируется).