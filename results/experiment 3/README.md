## Эксперимент №3: оценка влияния Positional Encoding и каузальной маски.

### Проблема

В задаче обработки естественного языка позиционное кодирование показало себя довольно полезным архитектурным решением. В задаче Next Token Prediction традиционно используется каузальная маска (или какая-то маска подобного рода). Нам интересно, какой эффект на результативность модели будут оказывать два таких элемента, в частности потому, что в предыдущих двух экспериментах позиционное кодирование не использовалось.

Я рассмотрю только стандартную каузальную маску с нулями выше диагонали и единицами во всех остальных позициях. В качестве позиционного кодирования я беру два метода: Sinusoidal Positional Encoding, который на мой взгляд кажется довольно натуральным, поскольку наш инпут представляет собой чередующиеся координаты и значения функций, и обучаемый PE, который используется в оригинальной статье и в архитектуре GPT2.

### Ожидаемый результат

Информация о позиции токенов окажется достаточно важной. Результат ухудшится, если убрать PE или каузальную маску, а если убрать и то, и другое, результат будет неудовлетворительным. SPE может показать результат лучше, чем обучаемый PE (LPE).

### Дизайн эксперимента

Я беру модель со следующей архитектурой:

| Параметр | Число слоёв | Головы внимания | Скрытая размерность | Размерность MLP слоя | Функция активации |
|---|:---:|:---:|:---:|:---:|:---:|
| Значение | 2 | 4 | 64 | 256 | GELU |

Каждую модель я рассматриваю в шести вариациях, и каждую вариацию обучаю при $b = 1$ и $b = 5$ с размером батча $64$, числом in-context примеров равным $63$ и $lr = 5e-4$.

### Результаты

Сравнение результатов работы моделей представлено на графике. Способ получения данных описан в README репозитория.

![Результат](./performance%201.svg)

Как мы видим, само по себе позиционное кодирование работает очень плохо, однако связка позиционного кодирования и каузальной маски значительно улучшает качество работы модели. Более того, как оказалось, обучаемый позиционный эмбеддинг (как у GPT2) даёт результаты лучшие, нежели синусоидальный.

Стоит также отметить, что при отсутствии каузальной маски и при наличии позиционного кодирования происходит значительное переобучение:

![Результат](./loss%20comparison.svg)

Я предполагаю, что это связано с тем, как устроена функция потерь: мы сравниваем сразу все токены, а не только последний, поэтому модель успешно минимизирует ошибку, но оказывается чрезвычайно неэффективной в целевой задаче.

На графике я усреднил каждую модель сразу и по $b$, и по зерну генерации (т.е. каждая линия соответствует шести различным запускам). Как мы можем видеть, обучение SPE + CM гораздо более нестабильно по сравнению с LPE + CM, а в целом SPE не даёт практически никакого выигрыша (по крайней мере этот выигрыш несколько маловероятен) по сравнению с обучением с просто каузальной маской.

Стоит также учитывать, что в нашем случае LPE добавляет примерно 8% дополнительных параметров. Кроме того, в будущем мы можем захотеть использовать ту же модель на последовательностях такой длины, которая будет отлична от длины в тренировочных данных. Это может ухудшить результат.

Можно также предположить, что для данной задачи может лучше всего подойти такое обучаемое позиционное кодирование, в котором обучается всего два токена - один соответствует координатам, другой - значениям функции. Эта идея почему-то пришла ко мне в голову не сразу, однако дедлайн уже подходил к концу и я решил отложить этот метод, хотя его определённо следует проверить.